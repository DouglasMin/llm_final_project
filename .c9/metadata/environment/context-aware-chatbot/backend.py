{"filter":false,"title":"backend.py","tooltip":"/context-aware-chatbot/backend.py","undoManager":{"mark":3,"position":3,"stack":[[{"start":{"row":0,"column":0},"end":{"row":43,"column":24},"action":"insert","lines":["import os","from langchain.memory import ConversationSummaryBufferMemory","from langchain.llms.bedrock import Bedrock","from langchain.chains import ConversationChain","","        ","def get_llm():","        ","    model_kwargs = { #AI21","        \"maxTokens\": 1024, ","        \"temperature\": 0.9, ","        \"topP\": 0.5, ","        \"stopSequences\": [\"Human:\"], ","        \"countPenalty\": {\"scale\": 0 }, ","        \"presencePenalty\": {\"scale\": 0 }, ","        \"frequencyPenalty\": {\"scale\": 0 } ","    }","    ","    llm = Bedrock(","        credentials_profile_name= 'default',","        model_id=\"ai21.j2-ultra-v1\", #set the foundation model","        model_kwargs=model_kwargs) #configure the properties for Claude","    ","    return llm","","","def get_memory(): #create memory for this chat session","    llm = get_llm()","    memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=512) #Maintains a summary of previous messages","    return memory","","","def get_chat_response(input_text, memory): #chat client function","    ","    llm = get_llm()","    conversation_with_summary = ConversationChain( #create a chat client","        llm = llm, #using the Bedrock LLM","        memory = memory, #with the summarization memory","        verbose = True #print out some of the internal states of the chain while running","    )","    ","    chat_response = conversation_with_summary.predict(input=input_text) #pass the user message and summary to the model","    ","    return chat_response"],"id":1}],[{"start":{"row":20,"column":33},"end":{"row":20,"column":34},"action":"remove","lines":["1"],"id":2},{"start":{"row":20,"column":32},"end":{"row":20,"column":33},"action":"remove","lines":["v"]},{"start":{"row":20,"column":31},"end":{"row":20,"column":32},"action":"remove","lines":["-"]},{"start":{"row":20,"column":30},"end":{"row":20,"column":31},"action":"remove","lines":["a"]},{"start":{"row":20,"column":29},"end":{"row":20,"column":30},"action":"remove","lines":["r"]},{"start":{"row":20,"column":28},"end":{"row":20,"column":29},"action":"remove","lines":["t"]},{"start":{"row":20,"column":27},"end":{"row":20,"column":28},"action":"remove","lines":["l"]},{"start":{"row":20,"column":26},"end":{"row":20,"column":27},"action":"remove","lines":["u"]},{"start":{"row":20,"column":25},"end":{"row":20,"column":26},"action":"remove","lines":["-"]},{"start":{"row":20,"column":24},"end":{"row":20,"column":25},"action":"remove","lines":["2"]},{"start":{"row":20,"column":23},"end":{"row":20,"column":24},"action":"remove","lines":["j"]},{"start":{"row":20,"column":22},"end":{"row":20,"column":23},"action":"remove","lines":["."]},{"start":{"row":20,"column":21},"end":{"row":20,"column":22},"action":"remove","lines":["1"]},{"start":{"row":20,"column":20},"end":{"row":20,"column":21},"action":"remove","lines":["2"]},{"start":{"row":20,"column":19},"end":{"row":20,"column":20},"action":"remove","lines":["i"]},{"start":{"row":20,"column":18},"end":{"row":20,"column":19},"action":"remove","lines":["a"]}],[{"start":{"row":20,"column":18},"end":{"row":20,"column":57},"action":"insert","lines":["anthropic.claude-3-sonnet-20240229-v1:0"],"id":3}],[{"start":{"row":0,"column":0},"end":{"row":43,"column":24},"action":"remove","lines":["import os","from langchain.memory import ConversationSummaryBufferMemory","from langchain.llms.bedrock import Bedrock","from langchain.chains import ConversationChain","","        ","def get_llm():","        ","    model_kwargs = { #AI21","        \"maxTokens\": 1024, ","        \"temperature\": 0.9, ","        \"topP\": 0.5, ","        \"stopSequences\": [\"Human:\"], ","        \"countPenalty\": {\"scale\": 0 }, ","        \"presencePenalty\": {\"scale\": 0 }, ","        \"frequencyPenalty\": {\"scale\": 0 } ","    }","    ","    llm = Bedrock(","        credentials_profile_name= 'default',","        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", #set the foundation model","        model_kwargs=model_kwargs) #configure the properties for Claude","    ","    return llm","","","def get_memory(): #create memory for this chat session","    llm = get_llm()","    memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=512) #Maintains a summary of previous messages","    return memory","","","def get_chat_response(input_text, memory): #chat client function","    ","    llm = get_llm()","    conversation_with_summary = ConversationChain( #create a chat client","        llm = llm, #using the Bedrock LLM","        memory = memory, #with the summarization memory","        verbose = True #print out some of the internal states of the chain while running","    )","    ","    chat_response = conversation_with_summary.predict(input=input_text) #pass the user message and summary to the model","    ","    return chat_response"],"id":4},{"start":{"row":0,"column":0},"end":{"row":38,"column":24},"action":"insert","lines":["import os","from langchain.memory import ConversationSummaryBufferMemory","from langchain_community.chat_models import BedrockChat","from langchain.chains import ConversationChain","","def get_llm():","    model_kwargs = {","        \"max_tokens\": 1024,  # Update the parameter name to match the one used by BedrockChat","        \"temperature\": 0.9,","        \"top_p\": 0.5,","        \"stop_sequences\": [\"Human:\"],","        \"count_penalty\": {\"scale\": 0},","        \"presence_penalty\": {\"scale\": 0},","        \"frequency_penalty\": {\"scale\": 0}","    }","","    llm = BedrockChat(","        credentials_profile_name='default',","        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # Set the foundation model","        model_kwargs=model_kwargs)  # Configure the properties for Claude","","    return llm","","def get_memory():  # create memory for this chat session","    llm = get_llm()","    memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=512)  # Maintains a summary of previous messages","    return memory","","def get_chat_response(input_text, memory):  # chat client function","    llm = get_llm()","    conversation_with_summary = ConversationChain(  # create a chat client","        llm=llm,  # using the Bedrock LLM","        memory=memory,  # with the summarization memory","        verbose=True  # print out some of the internal states of the chain while running","    )","","    chat_response = conversation_with_summary.predict(input=input_text)  # pass the user message and summary to the model","","    return chat_response"]}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":0,"selection":{"start":{"row":19,"column":39},"end":{"row":19,"column":39},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1717246895077,"hash":"5288b5c66f7c1b7660471d4ce6be2dc1c3394c48"}