{"filter":false,"title":"version1.py","tooltip":"/context-aware-chatbot/version1.py","undoManager":{"mark":7,"position":7,"stack":[[{"start":{"row":76,"column":0},"end":{"row":84,"column":38},"action":"remove","lines":["MODEL_OPTIONS = {","    \"Claude\" : \"anthropic.claude-3-sonnet-20240229-v1:0\",","    \"Llama\" : \"meta.llama3-8b-instruct-v1:0\",","    \"AI21\" : \"\",","    \"Amazon Titan\" : \"amazon.titan-embed-text-v1\"","}","        ","def format_func(model_choice):","    return MODEL_OPTIONS[model_choice]"],"id":923}],[{"start":{"row":78,"column":0},"end":{"row":79,"column":0},"action":"remove","lines":["",""],"id":924}],[{"start":{"row":87,"column":0},"end":{"row":91,"column":0},"action":"remove","lines":["    option = st.selectbox(","    \"what model would you like to use?\",","    options = list(MODEL_OPTIONS.keys()))","    st.write(\"You selected:\", format_func(option))",""],"id":925},{"start":{"row":86,"column":4},"end":{"row":87,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":98,"column":0},"end":{"row":98,"column":20},"action":"remove","lines":["    if streaming_on:"],"id":926}],[{"start":{"row":99,"column":0},"end":{"row":99,"column":4},"action":"remove","lines":["    "],"id":927},{"start":{"row":100,"column":0},"end":{"row":100,"column":4},"action":"remove","lines":["    "]},{"start":{"row":101,"column":0},"end":{"row":101,"column":4},"action":"remove","lines":["    "]},{"start":{"row":102,"column":0},"end":{"row":102,"column":4},"action":"remove","lines":["    "]},{"start":{"row":103,"column":0},"end":{"row":103,"column":4},"action":"remove","lines":["    "]},{"start":{"row":104,"column":0},"end":{"row":104,"column":4},"action":"remove","lines":["    "]},{"start":{"row":105,"column":0},"end":{"row":105,"column":4},"action":"remove","lines":["    "]},{"start":{"row":106,"column":0},"end":{"row":106,"column":4},"action":"remove","lines":["    "]}],[{"start":{"row":107,"column":0},"end":{"row":114,"column":0},"action":"remove","lines":["    else:","        with st.chat_message(\"assistant\"):","            # Get the response from the model without streaming","            response = get_streaming_response(conversation_history, chunk_handler)","            final_response = ''.join(response) if isinstance(response, list) else response","            st.write(final_response)","            st.session_state.messages.append({\"role\": \"assistant\", \"content\": final_response})",""],"id":928}],[{"start":{"row":84,"column":0},"end":{"row":85,"column":52},"action":"remove","lines":["    streaming_on = st.checkbox('Streaming')","    st.button('Clear Screen', on_click=clear_screen)"],"id":929},{"start":{"row":83,"column":48},"end":{"row":84,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":0,"column":0},"end":{"row":105,"column":0},"action":"remove","lines":["import streamlit as st","import json","import boto3","","# Bedrock runtime","bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")","","st.title(\"Chatbot powered by Bedrock with multi-choice model\")","","# Initialize session state if there are no messages","if \"messages\" not in st.session_state:","    st.session_state.messages = []","","# Display chat messages","for message in st.session_state.messages:","    with st.chat_message(message[\"role\"]):","        st.markdown(message[\"content\"])","","def clear_screen():","    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]","","def chunk_handler(chunk):","    text = \"\"","    chunk_type = chunk.get(\"type\")","    if chunk_type == \"message_start\":","        role = chunk[\"message\"][\"role\"]","        text = \"\"","    elif chunk_type == \"content_block_start\":","        text = chunk[\"content_block\"][\"text\"]","    elif chunk_type == \"content_block_delta\":","        text = chunk[\"delta\"][\"text\"]","    elif chunk_type == \"message_delta\":","        stop_reason = chunk[\"delta\"][\"stop_reason\"]","        text = \"\"","    elif chunk_type == \"message_stop\":","        metric = chunk[\"amazon-bedrock-invocationMetrics\"]","        inputTokenCount = metric[\"inputTokenCount\"]","        outputTokenCount = metric[\"outputTokenCount\"]","        firstByteLatency = metric[\"firstByteLatency\"]","        invocationLatency = metric[\"invocationLatency\"]","        text = \"\"","","    return text","","def get_streaming_response(messages, streaming_callback):","    try:","        body = json.dumps(","            {","                \"anthropic_version\": \"bedrock-2023-05-31\",","                \"max_tokens\": 1000,","                \"messages\": messages,","            }","        )","","        # stream","        response = bedrock_runtime.invoke_model_with_response_stream(","            modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",","            body=body,","        )","        stream = response.get(\"body\")","","        if stream:","            complete_response = \"\"","            for event in stream:  # Handle each event returned from the stream","                chunk = event.get(\"chunk\")","                if chunk:","                    chunk_json = json.loads(chunk.get(\"bytes\").decode())","                    if chunk_json is not None:","                        chunk_text = streaming_callback(chunk_json)","                        complete_response += chunk_text","                        yield complete_response  # Yield the accumulated response so far","            return complete_response","    except Exception as e:","        print(e, \"Error occurred!\")","        return \"\"","        ","","    ","","        ","# Sidebar","with st.sidebar:","    st.title('Streamlit Chat')","    st.subheader('With DynamoDB Memory :brain:')","    ","if user_prompt := st.chat_input(\"What's Up?\"):","    st.session_state.messages.append({\"role\": \"user\", \"content\": user_prompt})","    with st.chat_message(\"user\"):","        st.write(user_prompt)","    ","    # Prepare the conversation history for the model","    conversation_history = [","        {\"role\": msg[\"role\"], \"content\": [{\"type\": \"text\", \"text\": msg[\"content\"]}]}","        for msg in st.session_state.messages","    ]","    ","","    with st.chat_message(\"assistant\"):  ","        placeholder = st.empty()","        full_response = ''","        # Get the response from the model with streaming","        for chunk in get_streaming_response(conversation_history, chunk_handler):","            full_response = chunk","            placeholder.markdown(full_response)","        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})",""],"id":930},{"start":{"row":0,"column":0},"end":{"row":108,"column":0},"action":"insert","lines":["import streamlit as st","import json","import boto3","","# Bedrock runtime","bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\")","","st.title(\"Chatbot powered by Bedrock with multi-choice model\")","","# Initialize session state if there are no messages","if \"messages\" not in st.session_state:","    st.session_state.messages = []","    st.session_state.token_count = 0","","# Function to estimate the number of tokens in a message","def count_tokens(text):","    return len(text.split())","","# Display chat messages","for message in st.session_state.messages:","    with st.chat_message(message[\"role\"]):","        st.markdown(message[\"content\"])","","def clear_screen():","    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]","    st.session_state.token_count = 0","","def chunk_handler(chunk):","    text = \"\"","    chunk_type = chunk.get(\"type\")","    if chunk_type == \"message_start\":","        text = \"\"","    elif chunk_type == \"content_block_start\":","        text = chunk[\"content_block\"][\"text\"]","    elif chunk_type == \"content_block_delta\":","        text = chunk[\"delta\"][\"text\"]","    elif chunk_type == \"message_delta\":","        text = \"\"","    elif chunk_type == \"message_stop\":","        text = \"\"","    return text","","def get_streaming_response(messages, streaming_callback):","    try:","        body = json.dumps(","            {","                \"anthropic_version\": \"bedrock-2023-05-31\",","                \"max_tokens\": 1000,","                \"messages\": messages,","            }","        )","","        # stream","        response = bedrock_runtime.invoke_model_with_response_stream(","            modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",","            body=body,","        )","        stream = response.get(\"body\")","","        if stream:","            complete_response = \"\"","            for event in stream:  # Handle each event returned from the stream","                chunk = event.get(\"chunk\")","                if chunk:","                    chunk_json = json.loads(chunk.get(\"bytes\").decode())","                    if chunk_json is not None:","                        chunk_text = streaming_callback(chunk_json)","                        complete_response += chunk_text","                        yield complete_response  # Yield the accumulated response so far","            return complete_response","    except Exception as e:","        print(e, \"Error occurred!\")","        return \"\"","","# Sidebar","with st.sidebar:","    st.title('Streamlit Chat')","    st.subheader('With DynamoDB Memory :brain:')","    st.button('Clear Screen', on_click=clear_screen)","    st.write(f\"Total Tokens: {st.session_state.token_count}\")","    ","if user_prompt := st.chat_input(\"What's Up?\"):","    st.session_state.messages.append({\"role\": \"user\", \"content\": user_prompt})","    st.session_state.token_count += count_tokens(user_prompt)","    ","    with st.chat_message(\"user\"):","        st.write(user_prompt)","    ","    # Prepare the conversation history for the model","    conversation_history = [","        {\"role\": msg[\"role\"], \"content\": [{\"type\": \"text\", \"text\": msg[\"content\"]}]}","        for msg in st.session_state.messages","    ]","    ","    with st.chat_message(\"assistant\"):  ","        placeholder = st.empty()","        full_response = ''","        # Get the response from the model with streaming","        for chunk in get_streaming_response(conversation_history, chunk_handler):","            full_response = chunk","            placeholder.markdown(full_response)","        ","        st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})","        st.session_state.token_count += count_tokens(full_response)","","# Display updated token count in the sidebar","with st.sidebar:","    st.write(f\"Total Tokens: {st.session_state.token_count}\")",""]}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":8.399999999999999,"selection":{"start":{"row":37,"column":17},"end":{"row":37,"column":17},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1717322403303,"hash":"0b3c61f85b13406f58334cdf255542e5ca0fc776"}